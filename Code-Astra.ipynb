{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "\n",
    "from random import randint\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet34\n",
    "#device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "from custom_datasets import YogaPoseDataset\n",
    "\n",
    "import model_utils\n",
    "import plot_utils\n",
    "\n",
    "from imutils.video import VideoStream\n",
    "from scipy.spatial.distance import euclidean\n",
    "from imutils import perspective\n",
    "from imutils import contours\n",
    "import imutils\n",
    "from scipy.spatial import distance as dist\n",
    "from imutils.video import FPS\n",
    "import math\n",
    "import os\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import pyautogui\n",
    "from time import time\n",
    "from math import hypot\n",
    "import mediapipe as mp\n",
    "\n",
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from matplotlib.pyplot import specgram\n",
    "import scipy.io.wavfile\n",
    "import sys\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "import glob \n",
    "\n",
    "pyautogui.PAUSE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audioRecorder(): \n",
    "    CHUNK = 1024 \n",
    "    FORMAT = pyaudio.paInt16 #paInt8\n",
    "    CHANNELS = 2 \n",
    "    RATE = 44100 #sample rate\n",
    "    RECORD_SECONDS = 4\n",
    "    WAVE_OUTPUT_FILENAME = \"output11.wav\"\n",
    "\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK) #buffer\n",
    "\n",
    "    print(\"* recording\")\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data) # 2 bytes(16 bits) per channel\n",
    "\n",
    "    print(\"* done recording\")\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feelings = ['Calm', 'Happy', 'Sad', 'Angry', 'Fearful']\n",
    "def audioEmotionDetector(): \n",
    "    config = tensorflow.ConfigProto(\n",
    "    device_count={'GPU': 1},\n",
    "    intra_op_parallelism_threads=1,\n",
    "    allow_soft_placement=True\n",
    "    )\n",
    "\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.6\n",
    "\n",
    "    session = tensorflow.Session(config=config)\n",
    "    tf.keras.backend.set_session(session)\n",
    "\n",
    "    json_file = open('model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"saved_models/Emotion_Voice_Detection_Model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    data, sampling_rate = librosa.load('output10.wav')\n",
    "    X, sample_rate = librosa.load('output10.wav', res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
    "    sample_rate = np.array(sample_rate)\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13),axis=0)\n",
    "    featurelive = mfccs\n",
    "    livedf2 = featurelive\n",
    "    livedf2= pd.DataFrame(data=livedf2)\n",
    "    livedf2 = livedf2.stack().to_frame().T\n",
    "    twodim= np.expand_dims(livedf2, axis=2)\n",
    "    livepreds = loaded_model.predict(twodim, \n",
    "                             batch_size=32, \n",
    "                             verbose=1)\n",
    "    livepreds1=livepreds.argmax(axis=1)\n",
    "    liveabc = livepreds1.astype(int).flatten()\n",
    "    livefeeling = int(liveabc) - 2\n",
    "    feeling = feelings[livefeeling]\n",
    "    return feeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import model file and define pairs of pose\n",
    "protoFile = \"pose/coco/pose_deploy_linevec.prototxt\"\n",
    "weightsFile = \"pose/coco/pose_iter_440000.caffemodel\"\n",
    "nPoints = 18\n",
    "# COCO Output Format\n",
    "keypointsMapping = ['Nose', 'Neck', 'R-Sho', 'R-Elb', 'R-Wr', 'L-Sho', \n",
    "                    'L-Elb', 'L-Wr', 'R-Hip', 'R-Knee', 'R-Ank', 'L-Hip', \n",
    "                    'L-Knee', 'L-Ank', 'R-Eye', 'L-Eye', 'R-Ear', 'L-Ear']\n",
    "\n",
    "POSE_PAIRS = [[1,2], [1,5], [2,3], [3,4], [5,6], [6,7],\n",
    "              [1,8], [8,9], [9,10], [1,11], [11,12], [12,13],\n",
    "              [1,0], [0,14], [14,16], [0,15], [15,17],\n",
    "              [2,17], [5,16] ]\n",
    "\n",
    "# index of pafs correspoding to the POSE_PAIRS\n",
    "# e.g for POSE_PAIR(1,2), the PAFs are located at indices (31,32) of output, Similarly, (1,5) -> (39,40) and so on.\n",
    "mapIdx = [[31,32], [39,40], [33,34], [35,36], [41,42], [43,44], \n",
    "          [19,20], [21,22], [23,24], [25,26], [27,28], [29,30], \n",
    "          [47,48], [49,50], [53,54], [51,52], [55,56], \n",
    "          [37,38], [45,46]]\n",
    "\n",
    "colors = [ [0,100,255], [0,100,255], [0,255,255], [0,100,255], [0,255,255], [0,100,255],\n",
    "         [0,255,0], [255,200,100], [255,0,255], [0,255,0], [255,200,100], [255,0,255],\n",
    "         [0,0,255], [255,0,0], [200,200,0], [255,0,0], [200,200,0], [0,0,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the Keypoints using Non Maximum Suppression on the Confidence Map\n",
    "def getKeypoints(probMap, threshold=0.1):\n",
    "    \n",
    "    mapSmooth = cv2.GaussianBlur(probMap,(3,3),0,0)\n",
    "\n",
    "    mapMask = np.uint8(mapSmooth>threshold)\n",
    "    keypoints = []\n",
    "    \n",
    "    #find the blobs\n",
    "    contours, _ = cv2.findContours(mapMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    #contours,hierachy=cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    #for each blob find the maxima\n",
    "    for cnt in contours:\n",
    "        blobMask = np.zeros(mapMask.shape)\n",
    "        blobMask = cv2.fillConvexPoly(blobMask, cnt, 1)\n",
    "        maskedProbMap = mapSmooth * blobMask\n",
    "        _, maxVal, _, maxLoc = cv2.minMaxLoc(maskedProbMap)\n",
    "        keypoints.append(maxLoc + (probMap[maxLoc[1], maxLoc[0]],))\n",
    "\n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find valid connections between the different joints of a all persons present\n",
    "def getValidPairs(output):\n",
    "    valid_pairs = []\n",
    "    invalid_pairs = []\n",
    "    n_interp_samples = 10\n",
    "    paf_score_th = 0.1\n",
    "    conf_th = 0.7\n",
    "    # loop for every POSE_PAIR\n",
    "    for k in range(len(mapIdx)):\n",
    "        # A->B constitute a limb\n",
    "        pafA = output[0, mapIdx[k][0], :, :]\n",
    "        pafB = output[0, mapIdx[k][1], :, :]\n",
    "        pafA = cv2.resize(pafA, (frameWidth, frameHeight))\n",
    "        pafB = cv2.resize(pafB, (frameWidth, frameHeight))\n",
    "\n",
    "        # Find the keypoints for the first and second limb\n",
    "        candA = detected_keypoints[POSE_PAIRS[k][0]]\n",
    "        candB = detected_keypoints[POSE_PAIRS[k][1]]\n",
    "        nA = len(candA)\n",
    "        nB = len(candB)\n",
    "\n",
    "        # If keypoints for the joint-pair is detected\n",
    "        # check every joint in candA with every joint in candB \n",
    "        # Calculate the distance vector between the two joints\n",
    "        # Find the PAF values at a set of interpolated points between the joints\n",
    "        # Use the above formula to compute a score to mark the connection valid\n",
    "        \n",
    "        if( nA != 0 and nB != 0):\n",
    "            valid_pair = np.zeros((0,3))\n",
    "            for i in range(nA):\n",
    "                max_j=-1\n",
    "                maxScore = -1\n",
    "                found = 0\n",
    "                for j in range(nB):\n",
    "                    # Find d_ij\n",
    "                    d_ij = np.subtract(candB[j][:2], candA[i][:2])\n",
    "                    norm = np.linalg.norm(d_ij)\n",
    "                    if norm:\n",
    "                        d_ij = d_ij / norm\n",
    "                    else:\n",
    "                        continue\n",
    "                    # Find p(u)\n",
    "                    interp_coord = list(zip(np.linspace(candA[i][0], candB[j][0], num=n_interp_samples),\n",
    "                                            np.linspace(candA[i][1], candB[j][1], num=n_interp_samples)))\n",
    "                    # Find L(p(u))\n",
    "                    paf_interp = []\n",
    "                    for k in range(len(interp_coord)):\n",
    "                        paf_interp.append([pafA[int(round(interp_coord[k][1])), int(round(interp_coord[k][0]))],\n",
    "                                           pafB[int(round(interp_coord[k][1])), int(round(interp_coord[k][0]))] ]) \n",
    "                    # Find E\n",
    "                    paf_scores = np.dot(paf_interp, d_ij)\n",
    "                    avg_paf_score = sum(paf_scores)/len(paf_scores)\n",
    "                    \n",
    "                    # Check if the connection is valid\n",
    "                    # If the fraction of interpolated vectors aligned with PAF is higher then threshold -> Valid Pair  \n",
    "                    if ( len(np.where(paf_scores > paf_score_th)[0]) / n_interp_samples ) > conf_th :\n",
    "                        if avg_paf_score > maxScore:\n",
    "                            max_j = j\n",
    "                            maxScore = avg_paf_score\n",
    "                            found = 1\n",
    "                # Append the connection to the list\n",
    "                if found:            \n",
    "                    valid_pair = np.append(valid_pair, [[candA[i][3], candB[max_j][3], maxScore]], axis=0)\n",
    "\n",
    "            # Append the detected connections to the global list\n",
    "            valid_pairs.append(valid_pair)\n",
    "        else: # If no keypoints are detected\n",
    "            print(\"No Connection : k = {}\".format(k))\n",
    "            invalid_pairs.append(k)\n",
    "            valid_pairs.append([])\n",
    "    print(valid_pairs)\n",
    "    return valid_pairs, invalid_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates a list of keypoints belonging to each person\n",
    "# For each detected valid pair, it assigns the joint(s) to a person\n",
    "# It finds the person and index at which the joint should be added. This can be done since we have an id for each joint\n",
    "def getPersonwiseKeypoints(valid_pairs, invalid_pairs):\n",
    "    # the last number in each row is the overall score \n",
    "    personwiseKeypoints = -1 * np.ones((0, 19))\n",
    "\n",
    "    for k in range(len(mapIdx)):\n",
    "        if k not in invalid_pairs:\n",
    "            partAs = valid_pairs[k][:,0]\n",
    "            partBs = valid_pairs[k][:,1]\n",
    "            indexA, indexB = np.array(POSE_PAIRS[k])\n",
    "\n",
    "            for i in range(len(valid_pairs[k])): \n",
    "                found = 0\n",
    "                person_idx = -1\n",
    "                for j in range(len(personwiseKeypoints)):\n",
    "                    if personwiseKeypoints[j][indexA] == partAs[i]:\n",
    "                        person_idx = j\n",
    "                        found = 1\n",
    "                        break\n",
    "\n",
    "                if found:\n",
    "                    personwiseKeypoints[person_idx][indexB] = partBs[i]\n",
    "                    personwiseKeypoints[person_idx][-1] += keypoints_list[partBs[i].astype(int), 2] + valid_pairs[k][i][2]\n",
    "\n",
    "                # if find no partA in the subset, create a new subset\n",
    "                elif not found and k < 17:\n",
    "                    row = -1 * np.ones(19)\n",
    "                    row[indexA] = partAs[i]\n",
    "                    row[indexB] = partBs[i]\n",
    "                    # add the keypoint_scores for the two keypoints and the paf_score \n",
    "                    row[-1] = sum(keypoints_list[valid_pairs[k][i,:2].astype(int), 2]) + valid_pairs[k][i][2]\n",
    "                    personwiseKeypoints = np.vstack([personwiseKeypoints, row])\n",
    "    return personwiseKeypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directory of dataset\n",
    "JPEG_DIR = Path('dataset/')\n",
    "#Reading JSON file\n",
    "pose_list = json.load(Path('pose-list-with-meta.json').open())\n",
    "#Encode pose\n",
    "pose_id_to_name = {0: 'Bharadvajasana I', 1: 'Padangusthasana', 2: 'Paripurna Navasana', 3: 'Baddha Konasana', 4: 'Dhanurasana', 5: 'Setu Bandha Sarvangasana', 6: 'Ustrasana', 7: 'Marjaryasana', 8: 'Chakravakasana', 9: 'Ashtanga Namaskara', 10: 'Utkatasana', 11: 'Balasana', 12: 'Bhujangasana', 13: 'Savasana', 14: 'Gomukhasana', 15: 'Bitilasana', 16: 'Bakasana', 17: 'Makara Adho Mukha Svanasana', 18: 'Ardha Pincha Mayurasana', 19: 'Adho Mukha Svanasana', 20: 'Garudasana', 21: 'Sukhasana', 22: 'Astavakrasana', 23: 'Utthita Hasta Padangustasana', 24: 'Uttana Shishosana', 25: 'Utthita Parsvakonasana', 26: 'Utthita Trikonasana', 27: 'Pincha Mayurasana', 28: 'Agnistambhasana', 29: 'Tittibhasana', 30: 'Matsyasana', 31: 'Chaturanga Dandasana', 32: 'Malasana', 33: 'Parighasana', 34: 'Ardha Bhekasana', 35: 'Ardha Matsyendrasana', 36: 'Supta Matsyendrasana', 37: 'Ardha Chandrasana', 38: 'Adho Mukha Vriksasana', 39: 'Ananda Balasana', 40: 'Janu Sirsasana', 41: 'Virasana', 42: 'Krounchasana', 43: 'Utthita Ashwa Sanchalanasana', 44: 'Parsvottanasana', 45: 'Viparita Karani', 46: 'Salabhasana', 47: 'Natarajasana', 48: 'Padmasana', 49: 'Anjaneyasana', 50: 'Marichyasana III', 51: 'Hanumanasana', 52: 'Tadasana', 53: 'Pasasana', 54: 'Eka Pada Rajakapotasana', 55: 'Eka Pada Rajakapotasana II', 56: 'Mayurasana', 57: 'Kapotasana', 58: 'Phalakasana', 59: 'Halasana', 60: 'Eka Pada Koundinyanasana I', 61: 'Eka Pada Koundinyanasana II', 62: 'Marichyasana I', 63: 'Supta Baddha Konasana', 64: 'Supta Padangusthasana', 65: 'Supta Virasana', 66: 'Parivrtta Janu Sirsasana', 67: 'Parivrtta Parsvakonasana', 68: 'Parivrtta Trikonasana', 69: 'Tolasana', 70: 'Paschimottanasana', 72: 'Parsva Bakasana', 73: 'Vasisthasana', 74: 'Anantasana', 75: 'Salamba Bhujangasana', 76: 'Dandasana', 77: 'Uttanasana', 78: 'Ardha Uttanasana', 79: 'Urdhva Prasarita Eka Padasana', 80: 'Salamba Sirsasana', 81: 'Salamba Sarvangasana', 82: 'Vriksasana', 83: 'Urdhva Dhanurasana', 84: 'Dwi Pada Viparita Dandasana', 85: 'Purvottanasana', 86: 'Urdhva Hastasana', 87: 'Urdhva Mukha Svanasana', 88: 'Virabhadrasana I', 89: 'Virabhadrasana II', 90: 'Virabhadrasana III', 91: 'Upavistha Konasana', 92: 'Prasarita Padottanasana', 93: 'Camatkarasana', 94: 'Yoganidrasana', 95: 'Vrischikasana', 96: 'Vajrasana', 97: 'Tulasana', 98: 'Simhasana', 99: 'Makarasana', 100: 'Lolasana', 101: 'Kurmasana', 102: 'Garbha Pindasana', 103: 'Durvasasana', 71: 'Bhujapidasana', 104: 'Bhekasana', 105: 'Bhairavasana', 106: 'Ganda Bherundasana'}\n",
    "#Decode pose\n",
    "pose_name_to_id = {'bharadvajasana i': 0, 'padangusthasana': 1, 'paripurna navasana': 2, 'baddha konasana': 3, 'dhanurasana': 4, 'setu bandha sarvangasana': 5, 'ustrasana': 6, 'marjaryasana': 7, 'chakravakasana': 8, 'ashtanga namaskara': 9, 'utkatasana': 10, 'balasana': 11, 'bhujangasana': 12, 'savasana': 13, 'gomukhasana': 14, 'bitilasana': 15, 'bakasana': 16, 'makara adho mukha svanasana': 17, 'ardha pincha mayurasana': 18, 'adho mukha svanasana': 19, 'garudasana': 20, 'sukhasana': 21, 'astavakrasana': 22, 'utthita hasta padangustasana': 23, 'uttana shishosana': 24, 'utthita parsvakonasana': 25, 'utthita trikonasana': 26, 'pincha mayurasana': 27, 'agnistambhasana': 28, 'tittibhasana': 29, 'matsyasana': 30, 'chaturanga dandasana': 31, 'malasana': 32, 'parighasana': 33, 'ardha bhekasana': 34, 'ardha matsyendrasana': 35, 'supta matsyendrasana': 36, 'ardha chandrasana': 37, 'adho mukha vriksasana': 38, 'ananda balasana': 39, 'janu sirsasana': 40, 'virasana': 41, 'krounchasana': 42, 'utthita ashwa sanchalanasana': 43, 'parsvottanasana': 44, 'viparita karani': 45, 'salabhasana': 46, 'natarajasana': 47, 'padmasana': 48, 'anjaneyasana': 49, 'marichyasana iii': 50, 'hanumanasana': 51, 'tadasana': 52, 'pasasana': 53, 'eka pada rajakapotasana': 54, 'eka pada rajakapotasana ii': 55, 'mayurasana': 56, 'kapotasana': 57, 'phalakasana': 58, 'halasana': 59, 'eka pada koundinyanasana i': 60, 'eka pada koundinyanasana ii': 61, 'marichyasana i': 62, 'supta baddha konasana': 63, 'supta padangusthasana': 64, 'supta virasana': 65, 'parivrtta janu sirsasana': 66, 'parivrtta parsvakonasana': 67, 'parivrtta trikonasana': 68, 'tolasana': 69, 'paschimottanasana': 70, 'parsva bakasana': 72, 'vasisthasana': 73, 'anantasana': 74, 'salamba bhujangasana': 75, 'dandasana': 76, 'uttanasana': 77, 'ardha uttanasana': 78, 'urdhva prasarita eka padasana': 79, 'salamba sirsasana': 80, 'salamba sarvangasana': 81, 'vriksasana': 82, 'urdhva dhanurasana': 83, 'dwi pada viparita dandasana': 84, 'purvottanasana': 85, 'urdhva hastasana': 86, 'urdhva mukha svanasana': 87, 'virabhadrasana i': 88, 'virabhadrasana ii': 89, 'virabhadrasana iii': 90, 'upavistha konasana': 91, 'prasarita padottanasana': 92, 'camatkarasana': 93, 'yoganidrasana': 94, 'vrischikasana': 95, 'vajrasana': 96, 'tulasana': 97, 'simhasana': 98, 'makarasana': 99, 'lolasana': 100, 'kurmasana': 101, 'garbha pindasana': 102, 'durvasasana': 103, 'bhujapidasana': 71, 'bhekasana': 104, 'bhairavasana': 105, 'ganda bherundasana': 106}\n",
    "#Read classifier csv file\n",
    "classifier_csv = pd.read_csv('classifier_train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define resize method\n",
    "from PIL import ImageOps\n",
    "from PIL import Image\n",
    "\n",
    "class Resize(object):\n",
    "    def __init__(self, size=224):\n",
    "        self.size = size\n",
    "        \n",
    "    def __call__(self, im):  \n",
    "        if(im.height > im.width):\n",
    "            \n",
    "            w = int(self.size*im.width/im.height)\n",
    "            h = self.size\n",
    "            pad_val = int((224-w)/2)\n",
    "            pad = (224-w-pad_val,0,pad_val,0)\n",
    "        else:\n",
    "            h = int(self.size*im.height/im.width)\n",
    "            w = self.size\n",
    "            pad_val = int((224-h)/2)\n",
    "            pad = (0,224-h-pad_val,0,pad_val)\n",
    "        return ImageOps.expand(im.resize((w,h),resample=Image.BILINEAR), pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform\n",
    "bs = 32\n",
    "sz = 224\n",
    "n_epochs = 1\n",
    "lr = 0.001\n",
    "\n",
    "trn_tfms = transforms.Compose([\n",
    "    Resize(),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "val_tfms = transforms.Compose([\n",
    "    Resize(),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = transforms.ToPILImage()\n",
    "#p(trn_tfms(im))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define data loader\n",
    "dl = torch.utils.data.DataLoader(YogaPoseDataset('classifier_train_data.csv', trn_tfms, \n",
    "                                                           pose_id_to_name, pose_name_to_id),\n",
    "                                           bs, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model read\n",
    "model = resnet34(pretrained=True)\n",
    "model.fc = nn.Linear(512, 107)   \n",
    "model_utils.freeze_all_layers(model)\n",
    "#Making require_grade is true\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True   \n",
    "#Define Model\n",
    "model = resnet34(pretrained=True)\n",
    "model.fc = nn.Linear(512, 107)   \n",
    "#model = model.to(device)\n",
    "model.load_state_dict(torch.load('./models/yoga-asana-classifier.ckpt', map_location='cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define variable\n",
    "val_tfms = transforms.Compose([\n",
    "    Resize(),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to evaluate image on model\n",
    "def evaluate_single_image_on_model(model, val_tfms, test_img, pose_id_to_name):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        res = model(val_tfms(test_img)[None])\n",
    "        pred_probs = torch.nn.functional.softmax(res, dim=1)[0]\n",
    "        top_two = pred_probs.argsort()[-2:].numpy()\n",
    "        \n",
    "        res = []\n",
    "        \n",
    "        if(pred_probs[top_two[-1]] > 0.8):\n",
    "            res.append(pose_id_to_name[top_two[-1]])\n",
    "        else:\n",
    "            res.append(pose_id_to_name[top_two[-1]])\n",
    "            res.append(pose_id_to_name[top_two[-2]])\n",
    "        \n",
    "        return res\n",
    "        '''\n",
    "        res = []\n",
    "        if(pred_probs[top_two[-1]]>0.8):\n",
    "            res.append({'conf':round(pred_probs[top_two[-1]].item(), 2), 'pose_name': pose_id_to_name[top_two[-1]]})\n",
    "        else:\n",
    "            res.append({'conf':round(pred_probs[top_two[-1]].item(), 2), 'pose_name': pose_id_to_name[top_two[-1]]})\n",
    "            res.append({'conf':round(pred_probs[top_two[-2]].item(), 2), 'pose_name': pose_id_to_name[top_two[-2]]})\n",
    "        return res\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture():  \n",
    "    # initialize the camera\n",
    "    # If you have multiple camera connected with \n",
    "    # current device, assign a value in cam_port \n",
    "    # variable according to that\n",
    "    cam = cv2.VideoCapture(0)\n",
    "\n",
    "    # reading the input using the camera\n",
    "    result, image = cam.read()\n",
    "\n",
    "    # If image will detected without any error, \n",
    "    # show result\n",
    "    if result:\n",
    "\n",
    "        # showing result, it take frame name and image \n",
    "        # output\n",
    "        cv2.imshow(\"Yoga\", image)\n",
    "\n",
    "        # saving image in local storage\n",
    "        cv2.imwrite(\"YogaPose.png\", image)\n",
    "\n",
    "        # If keyboard interrupt occurs, destroy image \n",
    "        # window\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyWindow(\"Yoga\")\n",
    "\n",
    "    # If captured image is corrupted, moving to else part\n",
    "    else:\n",
    "        print(\"No image detected. Please! try again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open image\n",
    "def yogaPredictor():\n",
    "    test_img = Image.open('test5.png') # 'YogaPose.png'\n",
    "    # call evaluate method to show name of pose\n",
    "    yogaPose = evaluate_single_image_on_model(model, val_tfms, test_img, pose_id_to_name)\n",
    "    if yogaPose[0] == 'Bhujapidasana':\n",
    "        return \"Green\"\n",
    "    else:\n",
    "        return \"Red\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing db of food items across all canteens registered on the platform\n",
    "df1=pd.read_csv('food db/food.csv')\n",
    "df1.columns = ['food_id','title','canteen_id','price', 'num_orders', 'category', 'avg_rating', 'num_rating', 'tags']\n",
    "# mean of average ratings of all items\n",
    "C= df1['avg_rating'].mean()\n",
    "\n",
    "# the minimum number of votes required to appear in recommendation list, i.e, 60th percentile among 'num_rating'\n",
    "m= df1['num_rating'].quantile(0.6)\n",
    "\n",
    "# items that qualify the criteria of minimum num of votes\n",
    "q_items = df1.copy().loc[df1['num_rating'] >= m]\n",
    "\n",
    "# Calculation of weighted rating based on the IMDB formula\n",
    "def weighted_rating(x, m=m, C=C):\n",
    "    v = x['num_rating']\n",
    "    R = x['avg_rating']\n",
    "    return (v/(v+m) * R) + (m/(m+v) * C)\n",
    "\n",
    "# Applying weighted_rating to qualified items\n",
    "q_items['score'] = q_items.apply(weighted_rating, axis=1)\n",
    "\n",
    "# Shortlisting the top rated items and popular items\n",
    "top_rated_items = q_items.sort_values('score', ascending=False)\n",
    "pop_items= df1.sort_values('num_orders', ascending=False)\n",
    "\n",
    "# Display results of demographic filtering\n",
    "top_rated_items[['title', 'num_rating', 'avg_rating', 'score']].head()\n",
    "pop_items[['title', 'num_orders']].head()\n",
    "# Creating soup string for each item\n",
    "def create_soup(x):            \n",
    "    tags = x['tags'].lower().split(', ')\n",
    "    tags.extend(x['title'].lower().split())\n",
    "    tags.extend(x['category'].lower().split())\n",
    "    return \" \".join(sorted(set(tags), key=tags.index))\n",
    "\n",
    "df1['soup'] = df1.apply(create_soup, axis=1)\n",
    "df1.head(3)\n",
    "\n",
    "# Import CountVectorizer and create the count matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer(stop_words='english')\n",
    "\n",
    "# df1['soup']\n",
    "count_matrix = count.fit_transform(df1['soup'])\n",
    "\n",
    "# Compute the Cosine Similarity matrix based on the count_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_sim = cosine_similarity(count_matrix, count_matrix)\n",
    "\n",
    "indices_from_title = pd.Series(df1.index, index=df1['title'])\n",
    "indices_from_food_id = pd.Series(df1.index, index=df1['food_id'])\n",
    "\n",
    "# Function that takes in food title or food id as input and outputs most similar dishes \n",
    "def get_recommendations(title=\"\", cosine_sim=cosine_sim, idx=-1):\n",
    "    # Get the index of the item that matches the title\n",
    "    if idx == -1 and title != \"\":\n",
    "        idx = indices_from_title[title]\n",
    "\n",
    "    # Get the pairwsie similarity scores of all dishes with that dish\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the dishes based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get the scores of the 10 most similar dishes\n",
    "    sim_scores = sim_scores[1:3]\n",
    "\n",
    "    # Get the food indices\n",
    "    food_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Return the top 10 most similar dishes\n",
    "    return food_indices\n",
    "\n",
    "df1.loc[get_recommendations(title=\"Paneer Tikka\")]\n",
    "# fetch few past orders of a user, based on which personalized recommendations are to be made\n",
    "def get_latest_user_orders(user_id, orders, num_orders=3):\n",
    "    counter = num_orders\n",
    "    order_indices = []\n",
    "    \n",
    "    for index, row in orders[['user_id']].iterrows():\n",
    "        if row.user_id == user_id:\n",
    "            counter = counter -1\n",
    "            order_indices.append(index)\n",
    "        if counter == 0:\n",
    "            break\n",
    "            \n",
    "    return order_indices\n",
    "\n",
    "# utility function that returns a DataFrame given the food_indices to be recommended\n",
    "def get_recomms_df(food_indices, df1, columns, comment):\n",
    "    row = 0\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    for i in food_indices:\n",
    "        df.loc[row] = df1[['title', 'canteen_id', 'price']].loc[i]\n",
    "        df.loc[row].comment = comment\n",
    "        row = row+1\n",
    "    return df\n",
    "\n",
    "# return food_indices for accomplishing personalized recommendation using Count Vectorizer\n",
    "def personalised_recomms(orders, df1, user_id, columns, comment=\"based on your past orders\"):\n",
    "    order_indices = get_latest_user_orders(user_id, orders)\n",
    "    food_ids = []\n",
    "    food_indices = []\n",
    "    recomm_indices = []\n",
    "    \n",
    "    for i in order_indices:\n",
    "        food_ids.append(orders.loc[i].food_id)\n",
    "    for i in food_ids:\n",
    "        food_indices.append(indices_from_food_id[i])\n",
    "    for i in food_indices:\n",
    "        recomm_indices.extend(get_recommendations(idx=i))\n",
    "        \n",
    "    return get_recomms_df(set(recomm_indices), df1, columns, comment)\n",
    "\n",
    "# Simply fetch new items added by vendor or today's special at home canteen\n",
    "def get_new_and_specials_recomms(new_and_specials, users, df1, canteen_id, columns, comment=\"new/today's special item  in your home canteen\"):\n",
    "    food_indices = []\n",
    "    \n",
    "    for index, row in new_and_specials[['canteen_id']].iterrows():\n",
    "        if row.canteen_id == canteen_id:\n",
    "            food_indices.append(indices_from_food_id[new_and_specials.loc[index].food_id])\n",
    "            \n",
    "    return get_recomms_df(set(food_indices), df1, columns, comment)\n",
    "\n",
    "# utility function to get the home canteen given a user id\n",
    "def get_user_home_canteen(users, user_id):\n",
    "    for index, row in users[['user_id']].iterrows():\n",
    "        if row.user_id == user_id:\n",
    "            return users.loc[index].home_canteen\n",
    "    return -1\n",
    "\n",
    "# fetch items from previously calculated top_rated_items list\n",
    "def get_top_rated_items(top_rated_items, df1, columns, comment=\"top rated items across canteens\"):\n",
    "    food_indices = []\n",
    "    \n",
    "    for index, row in top_rated_items.iterrows():\n",
    "        food_indices.append(indices_from_food_id[top_rated_items.loc[index].food_id])\n",
    "        \n",
    "    return get_recomms_df(food_indices, df1, columns, comment)\n",
    "\n",
    "# fetch items from previously calculated pop_items list\n",
    "def get_popular_items(pop_items, df1, columns, comment=\"most popular items across canteens\"):\n",
    "    food_indices = []\n",
    "    \n",
    "    for index, row in pop_items.iterrows():\n",
    "        food_indices.append(indices_from_food_id[pop_items.loc[index].food_id])\n",
    "        \n",
    "    return get_recomms_df(food_indices, df1, columns, comment)\n",
    "    \n",
    "orders = pd.read_csv('food db/orders.csv')\n",
    "new_and_specials = pd.read_csv('food db/new_and_specials.csv')\n",
    "users = pd.read_csv('food db/users.csv')\n",
    "\n",
    "columns = ['title', 'price', 'comment']\n",
    "current_user = 2\n",
    "current_canteen = get_user_home_canteen(users, current_user)\n",
    "\n",
    "# get_popular_items(pop_items, df1, columns)\n",
    "\n",
    "# get_top_rated_items(top_rated_items, df1, columns)\n",
    "\n",
    "# get_new_and_specials_recomms(new_and_specials, users, df1, current_canteen, columns)\n",
    "\n",
    "# personalised_recomms(orders, df1, current_user, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('music.zip',compression='zip')\n",
    "data.drop_duplicates(inplace=True,subset=['name'])\n",
    "name=data['name']\n",
    "col_features = ['danceability', 'energy', 'valence', 'loudness']\n",
    "X = MinMaxScaler().fit_transform(data[col_features])\n",
    "kmeans = KMeans(init=\"k-means++\",\n",
    "                n_clusters=2,\n",
    "                random_state=15).fit(X)\n",
    "data['kmeans'] = kmeans.labels_\n",
    "data['song_name']=name\n",
    "cluster=data.groupby(by=data['kmeans'])\n",
    "y=data.pop('kmeans')\n",
    "x=data.drop(columns=['name','artists','id','release_date','song_name'])\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25)\n",
    "\n",
    "model=LGBMClassifier().fit(x_train,y_train)\n",
    "df=cluster.apply(lambda x: x.sort_values([\"popularity\"],ascending=False))\n",
    "df.reset_index(level=0, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(emotion_code):\n",
    "    NUM_RECOMMEND=10\n",
    "    happy_set=[]\n",
    "    sad_set=[]\n",
    "    if emotion_code==0:\n",
    "        happy_set.append(df[df['kmeans']==0]['song_name'].head(NUM_RECOMMEND))\n",
    "        return pd.DataFrame(happy_set).T\n",
    "    else:\n",
    "        sad_set.append(df[df['kmeans']==1]['song_name'].head(NUM_RECOMMEND))\n",
    "        return pd.DataFrame(sad_set).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def videoEmotionDetector():\n",
    "    model = model_from_json(open(\"fer.json\", \"r\").read())\n",
    "    model.load_weights('fer.h5')\n",
    "    face_haar_cascade = cv2.CascadeClassifier(r'haarcascade_frontalface_default.xml')\n",
    "    cap=cv2.VideoCapture(0)\n",
    "\n",
    "    while True:\n",
    "        ret,test_img=cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "        gray_img= cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)\n",
    "        for (x,y,w,h) in faces_detected:\n",
    "            cv2.rectangle(test_img,(x,y),(x+w,y+h),(255,0,0),thickness=7)\n",
    "            roi_gray=gray_img[y:y+w,x:x+h]\n",
    "            roi_gray=cv2.resize(roi_gray,(48,48))\n",
    "            img_pixels = image.img_to_array(roi_gray)\n",
    "            img_pixels = np.expand_dims(img_pixels, axis = 0)\n",
    "            img_pixels /= 255\n",
    "            predictions = model.predict(img_pixels)\n",
    "            max_index = np.argmax(predictions[0])\n",
    "\n",
    "            emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "            predicted_emotion = emotions[max_index]\n",
    "            cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "        resized_img = cv2.resize(test_img, (1000, 700))\n",
    "        cv2.imshow('Facial emotion analysis ',resized_img)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows\n",
    "    return predicted_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    " \n",
    "pose_image = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5, model_complexity=1)\n",
    " \n",
    "pose_video = mp_pose.Pose(static_image_mode=False, model_complexity=1, min_detection_confidence=0.7,\n",
    "                          min_tracking_confidence=0.7)\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_fps(image, time1):\n",
    "    time2 = time()\n",
    "    if (time2 - time1) > 0:\n",
    "        frames_per_second = 1.0 / (time2 - time1)\n",
    "        cv2.putText(image, 'FPS: {}'.format(int(frames_per_second)), (10, 30),cv2.FONT_HERSHEY_PLAIN, 2, (0, 255, 0), 3)\n",
    "    return time2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_pose(image, draw=False):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pose_video.process(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    if draw:\n",
    "        mp_drawing.draw_landmarks(image=image, landmark_list=results.pose_landmarks,\n",
    "                                  connections=mp_pose.POSE_CONNECTIONS,\n",
    "                                  landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255,255,255),\n",
    "                                                                               thickness=3, circle_radius=3),\n",
    "                                  connection_drawing_spec=mp_drawing.DrawingSpec(color=(49,125,237),\n",
    "                                                                               thickness=2, circle_radius=2))\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_game(image, landmarks):\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    left_wrist = int(landmarks[mp_pose.PoseLandmark.LEFT_WRIST].y * height)\n",
    "    right_wrist = int(landmarks[mp_pose.PoseLandmark.RIGHT_WRIST].y * height)\n",
    "    nose_pose = int(landmarks[mp_pose.PoseLandmark.NOSE].y * height)\n",
    "\n",
    "    JUMP_THRESH = 130\n",
    "    IDLE_THRESH = 290\n",
    "\n",
    "    control = 'idle'\n",
    "    if nose_pose < JUMP_THRESH and (left_wrist < IDLE_THRESH or right_wrist < IDLE_THRESH):\n",
    "        control = 'x'\n",
    "    elif right_wrist < IDLE_THRESH and left_wrist > right_wrist:\n",
    "        control = 'a'\n",
    "    elif left_wrist < IDLE_THRESH and left_wrist < right_wrist:\n",
    "        control = 's'\n",
    "    elif nose_pose < JUMP_THRESH:\n",
    "        control = 'up'\n",
    "    elif nose_pose > IDLE_THRESH:\n",
    "        control = 'down'\n",
    "\n",
    "    cv2.line(image, (0, JUMP_THRESH), (width - 1, JUMP_THRESH), (255, 255, 255), 2)\n",
    "    cv2.line(image, (0, IDLE_THRESH), (width - 1, IDLE_THRESH), (255, 255, 255), 2)\n",
    "    cv2.putText(image, 'ACTION: {}'.format(control), (400, 30),cv2.FONT_HERSHEY_PLAIN, 2, (255, 0, 0), 3)\n",
    "    return image, control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def press(key):\n",
    "    pyautogui.keyDown(key)\n",
    "    pyautogui.sleep(0.01)\n",
    "    pyautogui.keyUp(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def press_key(control, pause):\n",
    "    PAUSE_DURATION = 2\n",
    "    if control == 's' and pause <= 0:\n",
    "        press('s')\n",
    "        pause = PAUSE_DURATION\n",
    "    elif control == 'a' and pause <= 0:\n",
    "        press('a')\n",
    "        pause = PAUSE_DURATION\n",
    "    elif control == 'x' and pause <= 0:\n",
    "        press('x')\n",
    "        pause = PAUSE_DURATION\n",
    "    elif control == 'up' and pause <= 0:\n",
    "        press('up')\n",
    "        pause = PAUSE_DURATION\n",
    "    elif control == 'down' and pause <= 0:\n",
    "        press('down')\n",
    "        pause = PAUSE_DURATION\n",
    "    return pause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playGame1():\n",
    "    cap = cv2.VideoCapture(1)\n",
    "\n",
    "    cv2.namedWindow('Play Active', cv2.WINDOW_NORMAL)\n",
    "\n",
    "    time1 = time()\n",
    "    pause = 0\n",
    "    while cap.isOpened():\n",
    "\n",
    "        ok, frame = cap.read()\n",
    "\n",
    "        if not ok:\n",
    "            continue\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        # control = 's'\n",
    "\n",
    "        # frame_height, frame_width, _ = frame.shape\n",
    "        frame, results = detect_pose(frame, True)\n",
    "\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            frame, control = control_game(frame, landmarks)\n",
    "            pause = press_key(control, pause)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        time1 = put_fps(frame, time1)\n",
    "\n",
    "        cv2.imshow('Play Active', frame)\n",
    "\n",
    "        pause -= 1\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playGame2():\n",
    "    cap0 = cv2.VideoCapture(0)\n",
    "    \n",
    "    cap1 = cv2.VideoCapture(1)\n",
    "\n",
    "    time1 = time()\n",
    "    time2 = time()\n",
    "    pause = 0\n",
    "    while True:\n",
    "\n",
    "        ok0, frame0 = cap0.read()\n",
    "        ok1, frame1 = cap1.read()\n",
    "\n",
    "        frame0 = cv2.flip(frame0, 1)\n",
    "        frame1 = cv2.flip(frame1, 1)\n",
    "        # control = 's'\n",
    "\n",
    "        # frame_height, frame_width, _ = frame.shape\n",
    "        frame0, results0 = detect_pose(frame0, True)\n",
    "        frame1, results1 = detect_pose(frame1, True)\n",
    "\n",
    "        try:\n",
    "            landmarks0 = results0.pose_landmarks.landmark\n",
    "            frame0, control0 = control_game(frame0, landmarks0)\n",
    "            # pause = press_key(control0, pause)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            landmarks1 = results1.pose_landmarks.landmark\n",
    "            frame1, control1 = control_game(frame1, landmarks1)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        time1 = put_fps(frame0, time1)\n",
    "        time2 = put_fps(frame1, time2)\n",
    "\n",
    "        cv2.imshow('Play Active', frame0)\n",
    "        cv2.imshow('Player 2', frame1)\n",
    "\n",
    "        # pause -= 1\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap0.release()\n",
    "    cap1.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [07/Apr/2022 04:59:21] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [07/Apr/2022 04:59:23] \"\u001b[37mGET /gameZone HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [07/Apr/2022 04:59:23] \"\u001b[33mGET /js/jarallax.js HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [07/Apr/2022 04:59:23] \"\u001b[33mGET /js/responsiveslides.min.js HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [07/Apr/2022 04:59:23] \"\u001b[33mGET /js/move-top.js HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [07/Apr/2022 04:59:23] \"\u001b[33mGET /js/easing.js HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [07/Apr/2022 04:59:23] \"\u001b[33mGET /js/jquery.filterizr.js HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [07/Apr/2022 04:59:23] \"\u001b[33mGET /js/controls.js HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [07/Apr/2022 04:59:23] \"\u001b[33mGET /js/jquery.chocolat.js HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [07/Apr/2022 04:59:24] \"\u001b[33mGET /js/jarallax.js HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [07/Apr/2022 04:59:24] \"\u001b[33mGET /js/responsiveslides.min.js HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [07/Apr/2022 04:59:24] \"\u001b[33mGET /js/jquery.filterizr.js HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [07/Apr/2022 04:59:57] \"\u001b[33mGET /index.html HTTP/1.1\u001b[0m\" 404 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, redirect,url_for, request, render_template\n",
    "app=Flask(__name__,template_folder=r'C:\\Users\\Ishan\\Desktop\\AI\\template', static_folder=r'C:\\Users\\Ishan\\Desktop\\AI\\static')\n",
    "\n",
    "@app.route('/',methods=['POST','GET'])\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/recordAudio', methods = ['POST','GET'])\n",
    "def recorder():\n",
    "    audioRecorder()\n",
    "    return redirect(url_for('audioEmotion'))\n",
    "\n",
    "@app.route('/audioEmotion', methods=['POST','GET'])\n",
    "def audioEmotion():\n",
    "    emotionDetected = audioEmotionDetector()\n",
    "    print(emotionDetected)\n",
    "    return render_template('emotionDetected.html', emotion = emotionDetected)\n",
    "\n",
    "@app.route('/videoEmotion', methods=['POST','GET'])\n",
    "def videoEmotion():\n",
    "    emotionDetected = videoEmotionDetector()\n",
    "    print(emotionDetected)\n",
    "    return render_template('emotionDetected.html', emotion = emotionDetected)\n",
    "\n",
    "@app.route('/musicBase', methods=['POST', 'GET'])\n",
    "def musicBase():\n",
    "    return render_template('musicBase.html')\n",
    "\n",
    "@app.route('/music', methods = ['POST','GET'])\n",
    "def music():\n",
    "    # emotion_word = emotionDetected\n",
    "    emotion_word = 'happy'\n",
    "    if emotion_word=='sad':\n",
    "        emotion_code=0\n",
    "    else:\n",
    "        emotion_code=1\n",
    "    if emotion_word=='sad':\n",
    "        print('emotion detected is SAD')\n",
    "    else:\n",
    "        print('emotion detected is HAPPY')\n",
    "    musicRecommend = get_results(emotion_code)\n",
    "    musicRecommend.reset_index(drop=True, inplace=True)\n",
    "    return render_template('music.html', tables=[musicRecommend.to_html(classes='data', header='true')])\n",
    "\n",
    "@app.route('/yogaBase', methods = ['GET', 'POST'])\n",
    "def yogaBase():\n",
    "    return render_template('yogaBase.html')\n",
    "\n",
    "@app.route('/yoga', methods = ['POST', 'GET'])\n",
    "def yoga():\n",
    "    # time.sleep(5)\n",
    "    # capture()\n",
    "    yogaPosePredicted = yogaPredictor()\n",
    "    return render_template('yoga.html', yogaPose = yogaPosePredicted)\n",
    "    \n",
    "@app.route('/foodBase', methods = ['POST', 'GET'])   # Food Home Page\n",
    "def foodBase():\n",
    "    return render_template('foodBase.html')\n",
    "\n",
    "@app.route('/foodR1', methods = ['POST', 'GET'])\n",
    "def foodR1():\n",
    "    foodR1 = get_popular_items(pop_items, df1, columns)\n",
    "    return render_template('foodR1.html',  tables=[foodR1.to_html(classes='data', header=\"true\")])\n",
    "\n",
    "@app.route('/foodR2', methods = ['POST', 'GET'])\n",
    "def foodR2():\n",
    "    foodR2 = get_top_rated_items(top_rated_items, df1, columns)\n",
    "    return render_template('foodR2.html', tables =[foodR2.to_html(classes='data', header='true')])\n",
    "\n",
    "@app.route('/foodR3', methods = ['POST', 'GET'])\n",
    "def foodR3():\n",
    "    foodR3 = get_new_and_specials_recomms(new_and_specials, users, df1, current_canteen, columns)\n",
    "    return render_template('/foodR3.html', tables = [foodR3.to_html(classes='data', header='true')])\n",
    "\n",
    "@app.route('/foodR4', methods = ['POST', 'GET'])\n",
    "def foodR4():\n",
    "    foodR4 = personalised_recomms(orders, df1, current_user, columns)\n",
    "    return render_template('/foodR4.html', tables = [foodR4.to_html(classes='data', header = 'true')])\n",
    "\n",
    "@app.route('/gameZone', methods = ['POST','GET'])\n",
    "def gameZone():\n",
    "    return render_template('/gameZone.html')\n",
    "\n",
    "@app.route('/gamePlayer1', methods = ['POST','GET'])\n",
    "def gamePlayer1():\n",
    "    playGame1()\n",
    "    return render_template('/gameZone.html')\n",
    "\n",
    "@app.route('/gamePlayer2', methods = ['POST', 'GET'])\n",
    "def gamePlayer2():\n",
    "    playGame2()\n",
    "    return render_template('/gameZone.html')\n",
    "\n",
    "\n",
    "app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-1.13",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
